oPrediction <- new ("Prediction", input = sentence, output = combinedScoreTokens)
oPrediction
}
# ngram.intersect <- intersect( intersect(subGrams4$lastterm,subGrams3$lastterm),subGrams2$lastterm)
#
# o <- "???"
# t <- "???"
# gol <- function (v ) if ( length( v ) < 5 ) c <- length( v ) else 5
#
# if ( length( ngram.intersect ) > 0) {
#   t <- "ngram.intersect"
#   c <- gol( ngram.intersect )
#   o <- ngram.intersect[1:c]
# }
# else {
#   if ( nrow (subGrams4) > 0 ) {
#     t <- "ngram.4"
#     c <- gol( subGrams4$lastterm )
#     o <- subGrams4$lastterm[1:c]
#   }
#   else if ( nrow(subGrams3) > 0 ) {
#     t <- "ngram.3"
#     c <- gol( subGrams3$lastterm )
#     o <- subGrams3$lastterm[1:c]
#   }
#   else if ( nrow(subGrams2) > 0 ) {
#     t <- "ngram.2"
#     c <- gol( subGrams2$lastterm )
#     o <- subGrams2$lastterm[1:c]
#   }
#   else {
#     t <- "ngram.1"
#     c <- gol( subGrams1$lastterm )
#     o <- subGrams1$firstterms[1:c]
#   }
# }
p <- quizzer(q3, 8)
setClass("Prediction", slots = list(input = "character", output = "data.frame"))
subGrams1 <- dfGrams1
N <- sum(subGrams1$tf)
colnames(subGrams1)[colnames(subGrams1) == 'firstterms'] <- 'token'
subGrams1["type"] <- rep("ngram.1",nrow(subGrams1))
subGrams1["discount"] <- rep(1,nrow(subGrams1))
subGrams1["score"] <-  rep(0.0,nrow(subGrams1))
subGrams1$score <- subGrams1$tf / N
doPredict <- function ( sentence ) {
pc <- 1000
#print("<PREDICTION SETS 4, 3 and 2")
subGrams4 <- getNgramPredictionSet( sentence , 4, dfGrams4, pc )
subGrams3 <- getNgramPredictionSet( sentence , 3, dfGrams3, pc )
subGrams2 <- getNgramPredictionSet( sentence , 2, dfGrams2, pc )
# INNER FUNCTION: SCORE MULTIGRAMS ( > 3 )  (STUPID BACKOFF APPROACH)
scoreMultiGrams <- function ( gA, gB, backoff.discount ) {
countB <- sum( gB$tf )
print(countB)
gA$score <- ( gA$tf / countB ) * backoff.discount
gA
}
# INNER FUNCTION: SCORE BIGRAMS (STUPID BACKOFF APPROACH)
scoreBiGrams <- function ( gA, gB, backoff.discount ) {
for (term in gA$lastterm) {
countB <- gB[gB$token == term,]$tf
gA[gA$lastterm == term,]$score = (gA[gA$lastterm == term,]$tf / countB) * backoff.discount
}
gA
}
print(subGrams4)
scoredGrams4 <- scoreMultiGrams( subGrams4, subGrams3,  1)
print(scoredGrams4)
scoredGrams3 <- scoreMultiGrams( subGrams3, subGrams2,  0.4)
scoredGrams2 <- scoreBiGrams( head(subGrams2, 150), subGrams1,  (0.4 * 0.4))
combineTokens <- function ( combinedScoreTokens, scoredGrams) {
# print(str(scoredGrams))
if (nrow(scoredGrams) > 0) {
subScoredGrams <- subset(scoredGrams, select=c("lastterm", "score", "type"))
colnames(subScoredGrams)[colnames(subScoredGrams) == 'lastterm'] <- 'token'
rbind(combinedScoreTokens, subScoredGrams )
}
else {
combinedScoreTokens
}
}
# COMBINE SCORED TOKENS IN SINGLE DATA.FRAME
combinedScoreTokens <- data.frame(token=character(), score=numeric(), type=character(), stringsAsFactors=FALSE)
combinedScoreTokens <- combineTokens(combinedScoreTokens, scoredGrams4)
combinedScoreTokens <- combineTokens(combinedScoreTokens, scoredGrams3)
combinedScoreTokens <- combineTokens(combinedScoreTokens, scoredGrams2)
#combinedScoreTokens <- combineTokens(combinedScoreTokens, scoredGrams1)
#combinedScoreTokens <- rbind(combinedScoreTokens, subset(scoredGrams1, select=c("token", "score", "type")))
# ORDER BY SCORE DESC -> HIGHEST TO LOWEST
output <- head( combinedScoreTokens[with(combinedScoreTokens, order(score)), ], 20)
if (nrow(combinedScoreTokens) == 0) {
output <- head(subGrams1, 10)
}
oPrediction <- new ("Prediction", input = sentence, output = combinedScoreTokens)
oPrediction
}
# ngram.intersect <- intersect( intersect(subGrams4$lastterm,subGrams3$lastterm),subGrams2$lastterm)
#
# o <- "???"
# t <- "???"
# gol <- function (v ) if ( length( v ) < 5 ) c <- length( v ) else 5
#
# if ( length( ngram.intersect ) > 0) {
#   t <- "ngram.intersect"
#   c <- gol( ngram.intersect )
#   o <- ngram.intersect[1:c]
# }
# else {
#   if ( nrow (subGrams4) > 0 ) {
#     t <- "ngram.4"
#     c <- gol( subGrams4$lastterm )
#     o <- subGrams4$lastterm[1:c]
#   }
#   else if ( nrow(subGrams3) > 0 ) {
#     t <- "ngram.3"
#     c <- gol( subGrams3$lastterm )
#     o <- subGrams3$lastterm[1:c]
#   }
#   else if ( nrow(subGrams2) > 0 ) {
#     t <- "ngram.2"
#     c <- gol( subGrams2$lastterm )
#     o <- subGrams2$lastterm[1:c]
#   }
#   else {
#     t <- "ngram.1"
#     c <- gol( subGrams1$lastterm )
#     o <- subGrams1$firstterms[1:c]
#   }
# }
p <- quizzer(q3, 8)
p <- quizzer(q3, 8)
setClass("Prediction", slots = list(input = "character", output = "data.frame"))
subGrams1 <- dfGrams1
N <- sum(subGrams1$tf)
colnames(subGrams1)[colnames(subGrams1) == 'firstterms'] <- 'token'
subGrams1["type"] <- rep("ngram.1",nrow(subGrams1))
subGrams1["discount"] <- rep(1,nrow(subGrams1))
subGrams1["score"] <-  rep(0.0,nrow(subGrams1))
subGrams1$score <- subGrams1$tf / N
doPredict <- function ( sentence ) {
pc <- 1000
#print("<PREDICTION SETS 4, 3 and 2")
subGrams4 <- getNgramPredictionSet( sentence , 4, dfGrams4, pc )
subGrams3 <- getNgramPredictionSet( sentence , 3, dfGrams3, pc )
subGrams2 <- getNgramPredictionSet( sentence , 2, dfGrams2, pc )
# INNER FUNCTION: SCORE MULTIGRAMS ( > 3 )  (STUPID BACKOFF APPROACH)
scoreMultiGrams <- function ( gA, gB, backoff.discount ) {
countB <- sum( gB$tf )
print(countB)
gA$score <- ( gA$tf / countB ) * backoff.discount
gA
}
# INNER FUNCTION: SCORE BIGRAMS (STUPID BACKOFF APPROACH)
scoreBiGrams <- function ( gA, gB, backoff.discount ) {
for (term in gA$lastterm) {
countB <- gB[gB$token == term,]$tf
gA[gA$lastterm == term,]$score = (gA[gA$lastterm == term,]$tf / countB) * backoff.discount
}
gA
}
print(subGrams4)
scoredGrams4 <- scoreMultiGrams( subGrams4, subGrams3,  1)
print(scoredGrams4)
scoredGrams3 <- scoreMultiGrams( subGrams3, subGrams2,  0.4)
scoredGrams2 <- scoreBiGrams( head(subGrams2, 150), subGrams1,  (0.4 * 0.4))
combineTokens <- function ( combinedScoreTokens, scoredGrams) {
if (nrow(scoredGrams) > 0) {
subScoredGrams <- subset(scoredGrams, select=c("lastterm", "score", "type"))
colnames(subScoredGrams)[colnames(subScoredGrams) == 'lastterm'] <- 'token'
rbind(combinedScoreTokens, subScoredGrams )
}
else {
combinedScoreTokens
}
}
# COMBINE SCORED TOKENS IN SINGLE DATA.FRAME
combinedScoreTokens <- data.frame(token=character(), score=numeric(), type=character(), stringsAsFactors=FALSE)
combinedScoreTokens <- combineTokens(combinedScoreTokens, scoredGrams4)
combinedScoreTokens <- combineTokens(combinedScoreTokens, scoredGrams3)
combinedScoreTokens <- combineTokens(combinedScoreTokens, scoredGrams2)
#combinedScoreTokens <- combineTokens(combinedScoreTokens, scoredGrams1)
#combinedScoreTokens <- rbind(combinedScoreTokens, subset(scoredGrams1, select=c("token", "score", "type")))
# ORDER BY SCORE DESC -> HIGHEST TO LOWEST
output <- head( combinedScoreTokens[with(combinedScoreTokens, order(score)), ], 10)
if (nrow(output) == 0) {
output <- head(subGrams1, 10)
}
oPrediction <- new ("Prediction", input = sentence, output = output)
oPrediction
}
# ngram.intersect <- intersect( intersect(subGrams4$lastterm,subGrams3$lastterm),subGrams2$lastterm)
#
# o <- "???"
# t <- "???"
# gol <- function (v ) if ( length( v ) < 5 ) c <- length( v ) else 5
#
# if ( length( ngram.intersect ) > 0) {
#   t <- "ngram.intersect"
#   c <- gol( ngram.intersect )
#   o <- ngram.intersect[1:c]
# }
# else {
#   if ( nrow (subGrams4) > 0 ) {
#     t <- "ngram.4"
#     c <- gol( subGrams4$lastterm )
#     o <- subGrams4$lastterm[1:c]
#   }
#   else if ( nrow(subGrams3) > 0 ) {
#     t <- "ngram.3"
#     c <- gol( subGrams3$lastterm )
#     o <- subGrams3$lastterm[1:c]
#   }
#   else if ( nrow(subGrams2) > 0 ) {
#     t <- "ngram.2"
#     c <- gol( subGrams2$lastterm )
#     o <- subGrams2$lastterm[1:c]
#   }
#   else {
#     t <- "ngram.1"
#     c <- gol( subGrams1$lastterm )
#     o <- subGrams1$firstterms[1:c]
#   }
# }
p <- quizzer(q3, 8)
setClass("Prediction", slots = list(input = "character", output = "data.frame"))
subGrams1 <- dfGrams1
N <- sum(subGrams1$tf)
colnames(subGrams1)[colnames(subGrams1) == 'firstterms'] <- 'token'
subGrams1["type"] <- rep("ngram.1",nrow(subGrams1))
subGrams1["discount"] <- rep(1,nrow(subGrams1))
subGrams1["score"] <-  rep(0.0,nrow(subGrams1))
subGrams1$score <- subGrams1$tf / N
doPredict <- function ( sentence ) {
pc <- 1000
#print("<PREDICTION SETS 4, 3 and 2")
subGrams4 <- getNgramPredictionSet( sentence , 4, dfGrams4, pc )
subGrams3 <- getNgramPredictionSet( sentence , 3, dfGrams3, pc )
subGrams2 <- getNgramPredictionSet( sentence , 2, dfGrams2, pc )
# INNER FUNCTION: SCORE MULTIGRAMS ( > 3 )  (STUPID BACKOFF APPROACH)
scoreMultiGrams <- function ( gA, gB, backoff.discount ) {
countB <- sum( gB$tf )
print(countB)
gA$score <- ( gA$tf / countB ) * backoff.discount
gA
}
# INNER FUNCTION: SCORE BIGRAMS (STUPID BACKOFF APPROACH)
scoreBiGrams <- function ( gA, gB, backoff.discount ) {
for (term in gA$lastterm) {
countB <- gB[gB$token == term,]$tf
gA[gA$lastterm == term,]$score = (gA[gA$lastterm == term,]$tf / countB) * backoff.discount
}
gA
}
print(subGrams4)
scoredGrams4 <- scoreMultiGrams( subGrams4, subGrams3,  1)
print(scoredGrams4)
scoredGrams3 <- scoreMultiGrams( subGrams3, subGrams2,  0.4)
scoredGrams2 <- scoreBiGrams( head(subGrams2, 150), subGrams1,  (0.4 * 0.4))
combineTokens <- function ( combinedScoreTokens, scoredGrams) {
if (nrow(scoredGrams) > 0) {
subScoredGrams <- subset(scoredGrams, select=c("lastterm", "score", "type"))
colnames(subScoredGrams)[colnames(subScoredGrams) == 'lastterm'] <- 'token'
rbind(combinedScoreTokens, subScoredGrams )
}
else {
combinedScoreTokens
}
}
# COMBINE SCORED TOKENS IN SINGLE DATA.FRAME
combinedScoreTokens <- data.frame(token=character(), score=numeric(), type=character(), stringsAsFactors=FALSE)
combinedScoreTokens <- combineTokens(combinedScoreTokens, scoredGrams4)
combinedScoreTokens <- combineTokens(combinedScoreTokens, scoredGrams3)
combinedScoreTokens <- combineTokens(combinedScoreTokens, scoredGrams2)
#combinedScoreTokens <- combineTokens(combinedScoreTokens, scoredGrams1)
#combinedScoreTokens <- rbind(combinedScoreTokens, subset(scoredGrams1, select=c("token", "score", "type")))
# ORDER BY SCORE DESC -> HIGHEST TO LOWEST
output <- head( combinedScoreTokens[with(combinedScoreTokens, order(-score)), ], 10)
if (nrow(output) == 0) {
output <- head(subGrams1, 10)
}
oPrediction <- new ("Prediction", input = sentence, output = output)
oPrediction
}
# ngram.intersect <- intersect( intersect(subGrams4$lastterm,subGrams3$lastterm),subGrams2$lastterm)
#
# o <- "???"
# t <- "???"
# gol <- function (v ) if ( length( v ) < 5 ) c <- length( v ) else 5
#
# if ( length( ngram.intersect ) > 0) {
#   t <- "ngram.intersect"
#   c <- gol( ngram.intersect )
#   o <- ngram.intersect[1:c]
# }
# else {
#   if ( nrow (subGrams4) > 0 ) {
#     t <- "ngram.4"
#     c <- gol( subGrams4$lastterm )
#     o <- subGrams4$lastterm[1:c]
#   }
#   else if ( nrow(subGrams3) > 0 ) {
#     t <- "ngram.3"
#     c <- gol( subGrams3$lastterm )
#     o <- subGrams3$lastterm[1:c]
#   }
#   else if ( nrow(subGrams2) > 0 ) {
#     t <- "ngram.2"
#     c <- gol( subGrams2$lastterm )
#     o <- subGrams2$lastterm[1:c]
#   }
#   else {
#     t <- "ngram.1"
#     c <- gol( subGrams1$lastterm )
#     o <- subGrams1$firstterms[1:c]
#   }
# }
p <- quizzer(q3, 8)
p <- quizzer(q3, 7)
doPredict("How are you")
setClass("Prediction", slots = list(input = "character", output = "data.frame"))
subGrams1 <- dfGrams1
N <- sum(subGrams1$tf)
colnames(subGrams1)[colnames(subGrams1) == 'firstterms'] <- 'token'
subGrams1["type"] <- rep("ngram.1",nrow(subGrams1))
subGrams1["discount"] <- rep(1,nrow(subGrams1))
subGrams1["score"] <-  rep(0.0,nrow(subGrams1))
subGrams1$score <- subGrams1$tf / N
doPredict <- function ( sentence ) {
pc <- 1000
#print("<PREDICTION SETS 4, 3 and 2")
subGrams4 <- getNgramPredictionSet( sentence , 4, dfGrams4, pc )
subGrams3 <- getNgramPredictionSet( sentence , 3, dfGrams3, pc )
subGrams2 <- getNgramPredictionSet( sentence , 2, dfGrams2, pc )
# INNER FUNCTION: SCORE MULTIGRAMS ( > 3 )  (STUPID BACKOFF APPROACH)
scoreMultiGrams <- function ( gA, gB, backoff.discount ) {
countB <- sum( gB$tf )
print(countB)
gA$score <- ( gA$tf / countB ) * backoff.discount
gA
}
# INNER FUNCTION: SCORE BIGRAMS (STUPID BACKOFF APPROACH)
scoreBiGrams <- function ( gA, gB, backoff.discount ) {
for (term in gA$lastterm) {
countB <- gB[gB$token == term,]$tf
gA[gA$lastterm == term,]$score = (gA[gA$lastterm == term,]$tf / countB) * backoff.discount
}
gA
}
print(subGrams4)
scoredGrams4 <- scoreMultiGrams( subGrams4, subGrams3,  1)
print(scoredGrams4)
scoredGrams3 <- scoreMultiGrams( subGrams3, subGrams2,  0.4)
scoredGrams2 <- scoreBiGrams( head(subGrams2, 150), subGrams1,  (0.4 * 0.4))
combineTokens <- function ( combinedScoreTokens, scoredGrams) {
if (nrow(scoredGrams) > 0) {
subScoredGrams <- subset(scoredGrams, select=c("lastterm", "score", "type"))
colnames(subScoredGrams)[colnames(subScoredGrams) == 'lastterm'] <- 'token'
rbind(combinedScoreTokens, subScoredGrams )
}
else {
combinedScoreTokens
}
}
# COMBINE SCORED TOKENS IN SINGLE DATA.FRAME
combinedScoreTokens <- data.frame(token=character(), score=numeric(), type=character(), stringsAsFactors=FALSE)
combinedScoreTokens <- combineTokens(combinedScoreTokens, scoredGrams4)
combinedScoreTokens <- combineTokens(combinedScoreTokens, scoredGrams3)
combinedScoreTokens <- combineTokens(combinedScoreTokens, scoredGrams2)
#combinedScoreTokens <- combineTokens(combinedScoreTokens, scoredGrams1)
#combinedScoreTokens <- rbind(combinedScoreTokens, subset(scoredGrams1, select=c("token", "score", "type")))
# ORDER BY SCORE DESC -> HIGHEST TO LOWEST
output <- head( combinedScoreTokens[with(combinedScoreTokens, order(-score)), ], 10)
if (nrow(output) == 0) {
output <- head(subGrams1, 10)
}
oPrediction <- new ("Prediction", input = sentence, output = output)
oPrediction
}
# ngram.intersect <- intersect( intersect(subGrams4$lastterm,subGrams3$lastterm),subGrams2$lastterm)
#
# o <- "???"
# t <- "???"
# gol <- function (v ) if ( length( v ) < 5 ) c <- length( v ) else 5
#
# if ( length( ngram.intersect ) > 0) {
#   t <- "ngram.intersect"
#   c <- gol( ngram.intersect )
#   o <- ngram.intersect[1:c]
# }
# else {
#   if ( nrow (subGrams4) > 0 ) {
#     t <- "ngram.4"
#     c <- gol( subGrams4$lastterm )
#     o <- subGrams4$lastterm[1:c]
#   }
#   else if ( nrow(subGrams3) > 0 ) {
#     t <- "ngram.3"
#     c <- gol( subGrams3$lastterm )
#     o <- subGrams3$lastterm[1:c]
#   }
#   else if ( nrow(subGrams2) > 0 ) {
#     t <- "ngram.2"
#     c <- gol( subGrams2$lastterm )
#     o <- subGrams2$lastterm[1:c]
#   }
#   else {
#     t <- "ngram.1"
#     c <- gol( subGrams1$lastterm )
#     o <- subGrams1$firstterms[1:c]
#   }
# }
projectDir = "/Users/smallwes/develop/academic/coursera/datascience/c10-capstone/"
dataDir   <- paste0( projectDir, "data/")
funcDir   <- paste0( projectDir, "functions/")
wkDir <- paste0( projectDir, "week3/")
setwd(wkDir)
#set.seed("4789")
source( paste0(projectDir, "setup.R"))
source( paste0(funcDir, "globalUsePackage.R"))
usePackage("tm")
usePackage("SnowballC")
usePackage("quanteda")
usePackage("dplyr")
usePackage("tidyr")
#usePackage("RWeka")
# UPDATE CORPUS COMBINED DATASET
source(paste0(funcDir, "refreshCorpusCombinedDatasetFile.R"))
refreshCorpusCombinedDatasetFile( 0.20 )
# LOAD COMBINED DATASET DOCUMENT
filenameCombinedDataSubset <- paste0( dataDir, "en_US.combined_subset.txt" )
# LOAD AS TEXT DOCUMENT
doc <- readLines( filenameCombinedDataSubset, encoding = "UTF-8", skipNul = TRUE )
# LOAD AS CORPUS (TM PACKAGE)
#source(paste0(funcDir, "getSingleFileCorpus.R"))
#myCorpus <- getSingleFileCorpus( filenameCombinedDataSubset )
# --> PREPROCESS USING TM PACKAGE
#source(paste0(funcDir, "getCorpusPreprocessed.R"))
#myCorpus <- getCorpusPreprocessed( myCorpus )
# --> WRAP TM CORPUS %IN% QUANTEDA CORPUS
#doc <- corpus(x = myCorpus )
# LOAD PROFANITY WORDS
bannedWordList <- readLines( paste0( dataDir, "swearWords.txt" ) )
# USING QUANTEDA PACKAGE -> DFM FEATURE
getDFM <- function( d, n ) {
#dfm(d, ngrams = n, stem = TRUE, verbose = FALSE, ignoredFeatures = (bannedWordList, stopwords("english")))
dfm(d, ngrams = n, stem = FALSE, verbose = TRUE, ignoredFeatures = (bannedWordList))
#dfm(d, ngrams = n, stem = FALSE, verbose = TRUE)
}
dfmSparse1 <- getDFM(doc, 1 )
dfmSparse2 <- getDFM(doc, 2 )
dfmSparse3 <- getDFM(doc, 3 )
dfmSparse4 <- getDFM(doc, 4 )
# REMOVE SPARSE FEATURES USING TRIM QUANTEDA TRIM FUNCTION
#https://rdrr.io/cran/quanteda/man/trim.html
dfmSparse1tr <- trim(dfmSparse1, minCount = 2, minDoc = 2)
dfmSparse2tr <- trim(dfmSparse2, minCount = 2, minDoc = 2)
dfmSparse3tr <- trim(dfmSparse3, minCount = 2, minDoc = 2)
dfmSparse4tr <- trim(dfmSparse4, minCount = 2, minDoc = 2)
# GET TOP FEATURES
source(paste0(funcDir, "getTopFeaturesGrams.R"))
nHowManyFeatures <- 1000000
dfGrams1 <- getTopFeaturesGrams(dfmSparse1tr, ncol(dfmSparse1tr))
dfGrams2 <- getTopFeaturesGrams(dfmSparse2tr, ncol(dfmSparse2tr))
dfGrams3 <- getTopFeaturesGrams(dfmSparse3tr, ncol(dfmSparse3tr))
dfGrams4 <- getTopFeaturesGrams(dfmSparse4tr, ncol(dfmSparse4tr))
countUnigrams <- sum(dfGrams1$tf)
dfGrams1$score <- dfGrams1$tf / countUnigrams
# SANITY CHECK:
#dim(dfGrams1)
#dim(dfGrams2)
#dim(dfGrams3)
#dim(dfGrams4)
# PERFORM PREDICTIONS
source(paste0(funcDir, "doRemoveWords.R"))
source(paste0(funcDir, "doWordStemmer.R"))
source(paste0(funcDir, "getNgramPredictionSet.R"))
source(paste0(funcDir, "doPredict.R"))
source(paste0(projectDir, "tester_quiz_common.R"))
source(paste0(projectDir, "tester_quiz_2.R"))
source(paste0(projectDir, "tester_quiz_3.R"))
quizzer <- function( quiz, question ) {
resultPrediction <- doPredict(quiz[[question]]@question)
print(resultPrediction)
cat("--------------------------------------\n")
cat("ANSWERS:\n")
print(quiz[[question]]@answer)
}
quizzer( q3, 7)
test <- c("Hey's how are you doing'd can't piece would've I'd missy dee williams's won't")
gsub("'s", "", test)
gsub("won't", "will not", test)
stopwords("english")
projectDir = "/Users/smallwes/develop/academic/coursera/datascience/c10-capstone/"
dataDir   <- paste0( projectDir, "data/")
funcDir   <- paste0( projectDir, "functions/")
wkDir <- paste0( projectDir, "week3/")
setwd(wkDir)
#set.seed("4789")
source( paste0(projectDir, "setup.R"))
source( paste0(funcDir, "globalUsePackage.R"))
usePackage("tm")
usePackage("SnowballC")
usePackage("quanteda")
usePackage("dplyr")
usePackage("tidyr")
#usePackage("RWeka")
# UPDATE CORPUS COMBINED DATASET
source(paste0(funcDir, "refreshCorpusCombinedDatasetFile.R"))
refreshCorpusCombinedDatasetFile( 0.20 )
source(paste0(funcDir, "refreshCorpusCombinedDatasetFile.R"))
refreshCorpusCombinedDatasetFile( 0.20 )
